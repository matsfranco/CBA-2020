@book{intro_to_auto_robots,
 author = {Siegwart, Roland and Nourbakhsh, Illah R.},
 title = {Introduction to Autonomous Mobile Robots},
 year = {2004},
 isbn = {026219502X},
 publisher = {Bradford Company},
 address = {Scituate, MA, USA},
}

@misc{droneMovement,
author = {da Silva, Kleber Lima and de Morais, Aniel Silva},
pages = {1--22},
title = {{Hardware de Controle Avançado de Veículo Aéreo Não Tripulado do Tipo Quadricóptero}},
year = {2014}
}

@article{tuning_pid_bees,
	author = {Mohammed E. El-telbany},
	title = {Tuning PID Controller for DC Motor: An Artificial Bees Optimization Approach},
	journal = {International Journal of Computer Applications},
	year = {2013},
	volume = {77},
	number = {15},
	pages = {18-21},
	month = {September},
}

@article{pid_engine_tuning,
	author = {M.N. HOWELL, M.C. BEST},
	title = {On-line PID tuning for engine idle-speed control using continuous action reinforcement learning automata},
	journal = {Control Engineering Practice},
	year = {2000},
	volume = {8},
	number = {2},
	pages = {147-154},
	month = {September},
}

@misc{pid_autotune_relay,
   author = {Marco Gonçalo de Sousa Neves},
   title = {Auto-tuning de Controladores PID pelo método Relay: Optimização de Controlo em Automação Industrial},
   school = {Universidade Técnica de Lisboa},
   year = {2009},
 }
 
@misc {embarcados_pid_1,
	author = {Felipe Neves},
	title = {Controlador PID digital: Uma modelagem prática para microcontroladores - Parte 1},
	year = {2016},
	url = {https://www.embarcados.com.br/controlador-pid-digital-parte-1/},
	note = {Último acesso: 16/06/2018},
}

@misc {embarcados_pid_2,
	author = {Felipe Neves},
	title = {Controlador PID digital: Uma modelagem prática para microcontroladores - Parte 2},
	year = {2016},
	url = {https://www.embarcados.com.br/controlador-pid-digital-parte-2/},
	note = {Último acesso: 16/06/2018},
}

@book{des_emb_sist,
author = {Wilmshurst, Tim (University of Derby)},
file = {:C$\backslash$:/Users/mateu/Drive/Projetos/Online Tuning  for Path Following Controllers/Proposta de Projeto/Refer{\^{e}}ncias/Designing{\_}Embedded{\_}Systems{\_}with{\_}PIC{\_}Microcontrollers{\_}-{\_}Principles{\_}and{\_}Applications.pdf:pdf},
isbn = {9780750667555},
keywords = {Sistemas Embarcados},
mendeley-tags = {Sistemas Embarcados},
title = {{Designing Embedded Systems with PIC Microcontrollers (Principles and Applicatons)}},
year = {2007},
}

@article{path_plan_pid,
abstract = {This paper presents the development of a line follower wheeled mobile robot. In this project, LM3S811 which is ARM cortex-3 based microcontroller is chosen as the main controller to react towards the data received from infra-red line sensors to give fast, smooth, accurate and safe movement in partially structured environment. A dynamic PID control algorithm has been proposed to improve the navigation reliability of the wheeled mobile robot which uses differential drive locomotion system. The experimental results show that the dynamic PID algorithm can be performed under the system real-time requirements. {\textcopyright} 2012 IEEE.},
author = {Engin, Mustafa and Engin, Dilsad},
doi = {10.1109/EDERC.2012.6532213},
file = {:C$\backslash$:/Users/mateu/Drive/Projetos/Online Tuning  for Path Following Controllers/Proposta de Projeto/Refer{\^{e}}ncias/PATH PLANNING OF LINE FOLLOWER ROBOT.pdf:pdf},
isbn = {9781467345958},
journal = {EDERC 2012 - Proceedings of the 5th European DSP in Education and Research Conference},
keywords = {PID,PID control algorithm,Seguidor de Linha,embedded system,wheeled mobile robot},
mendeley-tags = {PID,Seguidor de Linha},
pages = {1--5},
title = {{Path planning of line follower robot}},
year = {2012},
}

@article{double_line,
author = {Pradesh, Andhra},
file = {:C$\backslash$:/Users/mateu/Drive/Projetos/Online Tuning  for Path Following Controllers/Proposta de Projeto/Refer{\^{e}}ncias/DESIGN{\_}AND{\_}IMPLEMENTATION{\_}OF{\_}DOUBLE{\_}LINE{\_}FOLLOWER{\_}.pdf:pdf},
keywords = {Seguidor de Linha,differential drive,ir sensors,line follower mobile robot,microcontroller,two-way path},
mendeley-tags = {Seguidor de Linha},
number = {6},
pages = {4946--4953},
title = {{Design and Implementation of Double Line Follower Robot}},
volume = {3},
year = {2011},
}

@article{lf_data_aq,
author = {Kaiser, F. and Islam, S. and Imran, W. and Khan, K. H. and Islam, K. M.A.},
doi = {10.1109/ICEEICT.2014.6919137},
file = {:C$\backslash$:/Users/mateu/Drive/Projetos/Online Tuning  for Path Following Controllers/Proposta de Projeto/Refer{\^{e}}ncias/Line Follower Robot Fabrication and accuracy.pdf:pdf},
isbn = {9781479948192},
journal = {1st International Conference on Electrical Engineering and Information and Communication Technology, ICEEICT 2014},
keywords = {Code,Data acquisition,Geared motor,Line follower,Microcontroller,Motor driver,Seguidor de Linha,Sensor board},
mendeley-tags = {Seguidor de Linha},
pages = {4--9},
title = {{Line follower robot: Fabrication and accuracy measurement by data acquisition}},
year = {2014},
}

@article{lf_line_sens_pos,
abstract = {Navigation is important to many envisioned applications of mobile robots. The variety of navigation tools may vary from expensive high accuracy tools to cheap low accuracy tools. The complexity of these tools would be dependent upon the navigation requirements. The more complex the navigation requirements, the more expensive the tools required. A cheap and simple navigation tool would be the line following sensor. However, the challenge posed in this navigation technique may be complex. A straight or wavy line would be simple to navigate whereas a T-junction, 90 degree bends and a grid junction would be difficult to navigate. This is due to the physical kinematics constraints which is limited to the motor response, position and the turning radius of the robot. This paper presents a proposed line sensor configuration to improve the navigation reliability of the differential drive line following robot.},
annote = {Estudo interessante sobre o posicionamento dos sensores},
author = {Baharuddin, M Zafri and Abidin, Izham Z and Mohideen, S Sulaiman Kaja},
file = {:C$\backslash$:/Users/mateu/Drive/Projetos/Online Tuning  for Path Following Controllers/Proposta de Projeto/Refer{\^{e}}ncias/Analysis of Line Sensor Configuration for the Advanced Line.pdf:pdf},
journal = {In Proc. of Student Conference on Research and Development (SCOReD), Bangi, Selangor, Malaysia.},
keywords = {Seguidor de Linha,adaptive programming,ldr sensors,wheeled mobile robot},
mendeley-tags = {Seguidor de Linha},
pages = {1--12},
title = {{Analysis of Line Sensor Configuration for the Advanced Line Follower Robot}},
year = {2006}
}

@article{mod_mat_cc,
author = {Thiago Zanon Gomes, André Fenili},
keywords = {identificação de paramentros, modelagem matemática, motor dc},
title = {{Modelagem Matemática e Identificação de Parâmetros de um Motor DC}},
volume = {97},
year = {2014}
}


@article{rl_rob_survey,
abstract = {Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Andrew Bagnell, J.},
doi = {10.1007/978-3-319-03194-1_2},
eprint = {9605103},
file = {:C$\backslash$:/Users/mateu/Drive/Projetos/Online Tuning  for Path Following Controllers/Proposta de Projeto/Refer{\^{e}}ncias/Reinforcement Learning in Robotics A Survey.pdf:pdf},
isbn = {9783642276446},
issn = {1610742X},
journal = {Springer Tracts in Advanced Robotics},
keywords = {Reinforcement Learning,learning control,reinforcement learning},
mendeley-tags = {Reinforcement Learning},
number = {September},
pages = {9--67},
pmid = {17255001},
primaryClass = {cs},
title = {{Reinforcement Learning in Robotics: A Survey}},
volume = {97},
year = {2014}
}

@article{intro_to_rl,
abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Sutton, R.S.; Barto, A. G.},
doi = {10.1016/S1364-6613(99)01331-5},
eprint = {1603.02199},
file = {:C$\backslash$:/Users/mateu/Drive/Projetos/Online Tuning  for Path Following Controllers/Proposta de Projeto/Refer{\^{e}}ncias/Reinforcement Learning An Introduction.pdf:pdf},
isbn = {0262193981},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
keywords = {Reinforcement Learning},
mendeley-tags = {Reinforcement Learning},
number = {9},
pages = {360},
pmid = {18255791},
title = {{Reinforcement Learning: An Introduction, by Sutton, R.S. and Barto, A.G.}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661399013315},
volume = {3},
year = {1999}
}

@article{ql_pid_robotics,
abstract = {Expert and intelligent systems are being developed to control many technological systems including mobile robots. However, the PID (Proportional-Integral-Derivative) controller is a fast low-level control strategy widely used in many control engineering tasks. Classic control theory has contributed with different tuning methods to obtain the gains of PID controllers for specific operation conditions. Nevertheless, when the system is not fully known and the operative conditions are variable and not previously known, classical techniques are not entirely suitable for the PID tuning. To overcome these drawbacks many adaptive approaches have been arisen, mainly from the field of artificial intelligent. In this work, we propose an incremental Q-learning strategy for adaptive PID control. In order to improve the learning efficiency we define a temporal memory into the learning process. While the memory remains invariant, a non-uniform specialization process is carried out generating new limited subspaces of learning. An implementation on a real mobile robot demonstrates the applicability of the proposed approach for a real-time simultaneous tuning of multiples adaptive PID controllers for a real system operating under variable conditions in a real environment.},
author = {Carlucho, Ignacio and {De Paula}, Mariano and Villar, Sebastian A. and Acosta, Gerardo G.},
doi = {10.1016/j.eswa.2017.03.002},
file = {:C$\backslash$:/Users/mateu/Drive/Projetos/Online Tuning  for Path Following Controllers/Proposta de Projeto/Refer{\^{e}}ncias/Incremental Q-learning strategy for adaptive PID control of mobile robots.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Incremental Q-learning,Mobile robots,Non-linear control,PID,Q-Learning,Reinforcement Learning,Reinforcement learning},
mendeley-tags = {Q-Learning,Reinforcement Learning},
pages = {183--199},
publisher = {Elsevier Ltd},
title = {{Incremental Q-learning strategy for adaptive PID control of mobile robots}},
url = {http://dx.doi.org/10.1016/j.eswa.2017.03.002},
volume = {80},
year = {2017}
}

@article{ql_model,
abstract = {{\textcopyright} 2015, Her Majesty the Queen in Right of Canada. This paper investigates reinforcement learning problems where a stochastic time delay is present in the reinforcement signal, but the delay is unknown to the learning agent. This work posits that the agent may receive individual reinforcements out of order, which is a relaxation of an important assumption in previous works from the literature. To that end, a stochastic time delay is introduced into a mobile robot line-following application. The main contribution of this work is to provide a novel stochastic approximation algorithm, which is an extension of Q-learning, for the time-delayed reinforcement problem. The paper includes a proof of convergence as well as grid world simulation results from MATLAB, results of line-following simulations within the Cyberbotics Webots mobile robot simulator, and finally, experimental results using an e-Puck mobile robot to follow a real track despite the presence of large, stochastic time delays in its reinforcement signal.},
author = {Campbell, Jeffrey S. and Givigi, Sidney N. and Schwartz, Howard M.},
doi = {10.1007/s10846-015-0222-2},
file = {:C$\backslash$:/Users/mateu/Drive/Projetos/Online Tuning  for Path Following Controllers/Proposta de Projeto/Refer{\^{e}}ncias/Multiple Model Q-Learning for Stochastic Asynchronous Rewards.pdf:pdf},
issn = {15730409},
journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
keywords = {Delayed reinforcement,Jitter,Machine Learning,Markov decision process,Multiple models,Q-Learning,Reinforcement Learning,Reinforcement learning,Stochastic time delay},
mendeley-tags = {Machine Learning,Q-Learning,Reinforcement Learning},
number = {3-4},
pages = {407--422},
title = {{Multiple Model Q-Learning for Stochastic Asynchronous Rewards}},
volume = {81},
year = {2016}
}

@article{markov_tut,
abstract = {H{\'{a}} situa{\c{c}}{\~{o}}es em que decis{\~{o}}es devem ser tomadas em seq{\"{u}}{\^{e}}ncia, e o resultado de cada decis{\~{a}}o n{\~{a}}o {\'{e}} claro para o tomador de decis{\~{o}}es. Estas situa{\c{c}}{\~{o}}es podem ser formuladas matematicamente como processos de decis{\~{a}}o de Markov, e dadas as probabilidades dos valores resultantes das decis{\~{o}}es, {\'{e}} poss{\'{i}}vel determinar uma pol{\'{i}}tica que maximize o valor esperado da seq{\"{u}}{\^{e}}ncia de decis{\~{o}}es. Este tutorial descreve os processos de decis{\~{a}}o de Markov (tanto o caso completamente observ{\'{a}}vel como o parcialmente observ{\'{a}}vel) e discute brevemente alguns m{\'{e}}todos para a sua solu{\c{c}}{\~{a}}o. Processos semi-Markovianos n{\~{a}}o s{\~{a}}o discutidos.},
author = {Pellegrini, J. and Wainer, Jacques},
file = {:C$\backslash$:/Users/mateu/Drive/Projetos/Online Tuning  for Path Following Controllers/Proposta de Projeto/Refer{\^{e}}ncias/Processo de Decis{\~{a}}o de Markov um tutorial.pdf:pdf},
issn = {21752745},
journal = {Revista de Inform{\'{a}}tica Te{\'{o}}rica e Aplicada},
keywords = {MDP,markov decicion processes,probabilistic,reasoning under uncertainty},
mendeley-tags = {MDP},
number = {2},
pages = {133--179},
title = {{Processos de Decis{\~{a}}o de Markov: um tutorial}},
url = {http://seer.ufrgs.br/index.php/rita/article/viewArticle/5694},
volume = {14},
year = {2008}
}

@book{comp_vis,
abstract = {Raspberry Pi was developed as a low-cost single-board computer with the intention of promoting computer science education in schools. It also represents a welcome return to a simple and fun yet effective way to learn computer science and programming. You can use Raspberry Pi to learn and implement concepts in computer vision. With a {\$}35 Raspberry Pi computer and a USB webcam, anyone can afford to become a pro in computer vision in no time and build a real-life computer vision application to impress friends and colleagues.},
annote = {[31]},
author = {Pajankar, Ashwin},
file = {:C$\backslash$:/Users/mateu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pajankar - 2015 - Raspberry Pi Computer Vision Programming.pdf:pdf},
isbn = {9781784398286},
keywords = {OpenCV,Visao Computacional},
mendeley-tags = {OpenCV,Visao Computacional},
pages = {178},
title = {{Raspberry Pi Computer Vision Programming}},
year = {2015}
}

@misc{ocv_ref,
  title={Open Source Computer Vision Library},
  author={Itseez},
  year={2018},
  howpublished = {\url{https://github.com/itseez/opencv}}
}

@article{incrementalQ,
abstract = {Expert and intelligent systems are being developed to control many technological systems including mobile robots. However, the PID (Proportional-Integral-Derivative) controller is a fast low-level control strategy widely used in many control engineering tasks. Classic control theory has contributed with different tuning methods to obtain the gains of PID controllers for specific operation conditions. Nevertheless, when the system is not fully known and the operative conditions are variable and not previously known, classical techniques are not entirely suitable for the PID tuning. To overcome these drawbacks many adaptive approaches have been arisen, mainly from the field of artificial intelligent. In this work, we propose an incremental Q-learning strategy for adaptive PID control. In order to improve the learning efficiency we define a temporal memory into the learning process. While the memory remains invariant, a non-uniform specialization process is carried out generating new limited subspaces of learning. An implementation on a real mobile robot demonstrates the applicability of the proposed approach for a real-time simultaneous tuning of multiples adaptive PID controllers for a real system operating under variable conditions in a real environment.},
author = {Carlucho, Ignacio and {De Paula}, Mariano and Villar, Sebastian A. and Acosta, Gerardo G.},
doi = {10.1016/j.eswa.2017.03.002},
file = {:C$\backslash$:/Users/mateu/Drive/Projetos/MATEUS FRANCO/Proposta de Projeto/Refer{\^{e}}ncias/Incremental Q-learning strategy for adaptive PID control of mobile robots.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Incremental Q-learning,Mobile robots,Non-linear control,PID,Q-Learning,Reinforcement Learning,Reinforcement learning},
mendeley-tags = {Q-Learning,Reinforcement Learning},
pages = {183--199},
publisher = {Elsevier Ltd},
title = {{Incremental Q-learning strategy for adaptive PID control of mobile robots}},
url = {http://dx.doi.org/10.1016/j.eswa.2017.03.002},
volume = {80},
year = {2017}
}

@article{asyncRwd,
abstract = {{\textcopyright} 2015, Her Majesty the Queen in Right of Canada. This paper investigates reinforcement learning problems where a stochastic time delay is present in the reinforcement signal, but the delay is unknown to the learning agent. This work posits that the agent may receive individual reinforcements out of order, which is a relaxation of an important assumption in previous works from the literature. To that end, a stochastic time delay is introduced into a mobile robot line-following application. The main contribution of this work is to provide a novel stochastic approximation algorithm, which is an extension of Q-learning, for the time-delayed reinforcement problem. The paper includes a proof of convergence as well as grid world simulation results from MATLAB, results of line-following simulations within the Cyberbotics Webots mobile robot simulator, and finally, experimental results using an e-Puck mobile robot to follow a real track despite the presence of large, stochastic time delays in its reinforcement signal.},
author = {Campbell, Jeffrey S. and Givigi, Sidney N. and Schwartz, Howard M.},
doi = {10.1007/s10846-015-0222-2},
file = {:C$\backslash$:/Users/mateu/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Campbell, Givigi, Schwartz - 2016 - Multiple Model Q-Learning for Stochastic Asynchronous Rewards.pdf:pdf},
issn = {15730409},
journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
keywords = {Delayed reinforcement,Introdu{\c{c}}{\~{a}}o,Jitter,Machine Learning,Markov decision process,Multiple models,Q-Learning,Reinforcement Learning,Reinforcement learning,Stochastic time delay,Trabalhos Relacionados},
mendeley-tags = {Introdu{\c{c}}{\~{a}}o,Machine Learning,Q-Learning,Reinforcement Learning,Trabalhos Relacionados},
number = {3-4},
pages = {407--422},
title = {{Multiple Model Q-Learning for Stochastic Asynchronous Rewards}},
volume = {81},
year = {2016}
}

@misc {hallEffectAndEncoders,
	author = {Giovana Monari},
	title = {Understanding Resolution in Optical and Magnetic Encoders},
	year = {2013},
	url = {//www.electronicdesign.com/components/understanding-resolution-optical-and-magnetic-encoders},
	note = {Último acesso: 16/06/2018},
}

@book{halliday,
  title={(WCS)Fundamentals of Physics Extended, Eighth Edition Binder Ready Version},
  author={Halliday, M.A.K. and Halliday, D.},
  isbn={9780470176214},
  url={https://books.google.com.br/books?id=4TD2uAAACAAJ},
  year={2007},
  publisher={John Wiley \& Sons Canada, Limited}
}


@misc {mcuDevelopmentReference,
	author = {Andrew Kramer},
	title = {Motor Encoders with Arduino},
	year = {2016},
	url = {http://andrewjkramer.net/motor-encoders-arduino/},
	note = {Último acesso: 16/06/2018},
}


@misc {pidImprovements,
	author = {Brett Beauregard},
	title = {Improving the Beginner’s PID},
	year = {2012},
	url = {http://brettbeauregard.com/blog/2011/04/improving-the-beginners-pid-introduction/},
	note = {Último acesso: 16/06/2018},
}

@misc {spotMini,
	author = {Rafael Carneiro},
	title = {Boston Dynamics planeja fabricar mil robôs SpotMini por ano},
	year = {2018},
	url = {https://epocanegocios.globo.com/Empresa/noticia/2018/07/boston-dynamics-planeja-fabricar-mil-robos-spotmini-por-ano.html},
	note = {Último acesso: 01/11/2018},
}

@misc {mavic,
	author = {Drew Prindle},
	title = {Alibaba’s favorite warehouse robot maker gets funding},
	year = {2017},
	url = {https://www.digitaltrends.com/drone-reviews/dji-mavic-pro-review/},
	note = {Último acesso: 01/11/2018},
}

@misc {alibaba,
	author = {Steven Millward},
	title = {DJI Mavic Pro review},
	year = {2018},
	url = {https://www.techinasia.com/china-warehouse-robots-startup-geek-funding},
	note = {Último acesso: 01/11/2018},
}

@misc {openrov,
	author = {Patrick Meier},
	title = {Could These Swimming Robots Help Local Communities?},
	year = {2016},
	url = {https://blog.werobotics.org/2016/08/03/could-these-swimming-robots-help-local-communities/},
	note = {Último acesso: 01/11/2018},
}

@article{nonLinearPID,
  title={Non-linear model predictive control schemes with application on a 2 link vertical robot manipulator},
  author={Wilson, Jacob and Charest, Meaghan and Dubay, Rickey},
  journal={Robotics and Computer-Integrated Manufacturing},
  volume={41},
  pages={23--30},
  year={2016},
  publisher={Elsevier}
}

@article{qLearningDev,
  title={Q-Learning for Adaptive PID Control of a Line Follower Mobile Robot},
  year={2017},
  author={Gurel, CANBERK SUAT}
}

@INPROCEEDINGS{Watkins92q-learning,
    author = {Christopher J. C. H. Watkins and Peter Dayan},
    title = {Q-learning},
    booktitle = {Machine Learning},
    year = {1992},
    pages = {279--292}
}